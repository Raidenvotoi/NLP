import streamlit as st
import re
from io import StringIO
# Import necessary modules
import pandas as pd
import plotly.express as px
import requests
from bs4 import BeautifulSoup
from re import findall
import time
from nltk.corpus import wordnet
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import contractions
from spellchecker import SpellChecker
import nltk
from nltk import pos_tag, word_tokenize, RegexpParser
from nltk import pos_tag
from nltk import word_tokenize
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from IPython.display import display
import string
from googletrans import Translator
import random
from langdetect import detect, detect_langs
from collections import Counter
import seaborn as sns
import spacy
import networkx as nx
# from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from datasets import load_dataset
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score
import praw
from datasets import load_dataset
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score
from transformers import BertTokenizer, BertModel
from transformers import AutoModel, AutoTokenizer
import torch
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier  # N·∫øu mu·ªën d√πng c√¢y quy·∫øt ƒë·ªãnh
from sklearn.ensemble import RandomForestClassifier  # ƒê√∫ng c√°ch
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import OneHotEncoder
import streamlit_antd_components as sac
import random
from deep_translator import GoogleTranslator
import json 
from streamlit_elements import elements, mui, html


def translate_and_back(text, num_languages=10):
    """
    Translates a paragraph into multiple languages and back.

    Args:
        text: The input paragraph.
        num_languages: The number of languages to translate to.

    Returns:
        str: The translated paragraph.
    """

    # Split the paragraph into sentences.
    sentences = text.split(".")

    # Randomly select languages for each sentence.
    # Create an instance of GoogleTranslator to call get_supported_languages()
    translator_instance = GoogleTranslator()
    languages = random.choices(
        translator_instance.get_supported_languages(), k=num_languages)

    # Translate each sentence to a different language and back.
    translated_sentences = []
    for sentence in sentences:
        language = random.choice(languages)
        translator = GoogleTranslator(source='auto', target=language)
        translated_sentence = translator.translate(sentence)
        translator_back = GoogleTranslator(source=language, target='auto')
        translated_back_sentence = translator_back.translate(
            translated_sentence)
        translated_sentences.append(translated_back_sentence)

    # Join the translated sentences back together.
    translated_paragraph = ". ".join(translated_sentences)

    return translated_paragraph


def remove_stopwords(text):
    stop_words = set(stopwords.words("english"))
    word_tokens = word_tokenize(text.lower())  # Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    filtered_text = " ".join(
        word for word in word_tokens if word not in stop_words)
    return filtered_text


def get_comment(soup):
    soupcmt = soup
    article_cmt = soupcmt.find_all('article', 'user-review-item')
    cmt = []
    # print(article_cmt)
    for i in article_cmt:
        try:
            # print("hi")
            title = i.find("h3", "ipc-title__text").text
            # print(title)
            content = i.find("div", "ipc-html-content-inner-div").text
            star = i.find("span", "ipc-rating-star--rating").text
            cmt.append([title, star, content])
        except:
            continue
    return cmt


def remove_numbers(text):
    result = re.sub(r'\d+', '', text)
    return result


def remove_whitespace(text):
    return " ".join(text.split())


def remove_punctuation(text):
    # translator = str.maketrans('', '', string.punctuation)
    return text.translate(str.maketrans("", "", string.punctuation))


def clean_text(text):
    if clean_lower:
        text = text.lower()
    if clean_ExpandingContractions:
        text = contractions.fix(text)
    if clean_PunctuationRemoval:
        text = text.translate(str.maketrans("", "", string.punctuation))

    if clean_numbers:
        text = re.sub(r"\d+", "", text)

    if clean_spaces:
        text = " ".join(text.split())

    if clean_StopWordsRemoval:
        # print("xoa tu dung ")
        stop_words = set(stopwords.words("english"))
        words = word_tokenize(text)
        text = " ".join(word for word in words if word not in stop_words)
        # print(text)
    if clean_symbols:
        text = re.sub(r"[^\w\s]", "", text)

    if clean_Stemming:
        stemmer = PorterStemmer()
        words = word_tokenize(text)
        text = " ".join(stemmer.stem(word) for word in words)

    if clean_Lemmatization:
        lemmatizer = WordNetLemmatizer()
        words = word_tokenize(text)
        text = " ".join(lemmatizer.lemmatize(word) for word in words)

    if clean_specialchar:
        listcau = text.split(".")
        rejoined_text = []  # Initialize an empty list to store processed sentences
        for sentence in listcau:
            # Apply substitution to each sentence
            processed_sentence = re.sub(r"[^a-zA-Z0-9\s]", "", sentence)
            # Add processed sentence to the list
            rejoined_text.append(processed_sentence)

        text = ".".join(rejoined_text)
    return text
from dotenv import load_dotenv
import os

load_dotenv()

def crawl_text(limit, subreddit):
    client_id = os.getenv("CLIENT_ID")
    client_secret = os.getenv("CLIENT_SECRET")
    user_agent = os.getenv("USER_AGENT")
    reddit = praw.Reddit(
        client_id=client_id,
        client_secret=client_secret,
        user_agent=user_agent
    )
    subreddit = reddit.subreddit(subreddit)
    u = 0
    ups = []
    text = []
    title = []
    for submission in subreddit.hot(limit=limit):
        text.append(submission.selftext.replace('\r', '').replace('\n', ' '))
        ups.append(submission.ups)
        title.append(submission.title)

    data = {'title': title, 'ups': ups, 'text': text, }

    return pd.DataFrame.from_dict(data)
# def input_json(path):
    

def predict_sentiment(text, pipline_cls):

    label = pipline_cls.predict([text])[0]
    return "T√≠ch c·ª±c" if label == 1 else "Ti√™u c·ª±c"


def predict_topic(text, pipline_cls):

    label = pipline_cls.predict([text])[0]
    if label == 0:
        label = "World"
    elif label == 1:
        label = "Sports"
    elif label == 2:
        label = "Business"
    elif label == 3:
        label = "Sci/Tech"
    return label


def predict_question(text, pipline_cls):

    label = pipline_cls.predict([text])[0]
    if label == 0:
        label = "Vi·∫øt t·∫Øt"
    elif label == 1:
        label = "Th·ª±c th·ªÉ"
    elif label == 2:
        label = "M√¥ t·∫£ v√† kh√°i ni·ªám tr·ª´u t∆∞·ª£ng."
    elif label == 3:
        label = "Con ng∆∞·ªùi"
    elif label == 4:
        label = "V·ªã tr√≠"
    elif label == 5:
        label = "Gi√° tr·ªã s·ªë"

    return label


# if "df3" in st.session_state:
st.title("X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n")


def synonym_replacement_EN(text, n=2):
    # text=df['text']
    words = word_tokenize(text)
    new_words = words.copy()
    random_words = list(set(words))
    random.shuffle(random_words)

    num_replaced = 0
    for word in random_words:
        synonyms = wordnet.synsets(word)
        if synonyms:
            synonym = synonyms[0].lemmas()[0].name()
            if synonym != word:
                new_words = [synonym if w == word else w for w in new_words]
                num_replaced += 1
            if num_replaced >= n:
                break
    return ' '.join(new_words)


def word_shuffling(text):
    words = text.split()
    random.shuffle(words)
    return ' '.join(words)


def noise_injection(text, probability=0.1):
    words = list(text)  # Chuy·ªÉn chu·ªói th√†nh danh s√°ch k√Ω t·ª±
    for i in range(len(words)):  # Duy·ªát t·ª´ng k√Ω t·ª±
        if random.random() < probability:  # X√°c su·∫•t thay th·∫ø k√Ω t·ª±
            # Thay b·∫±ng k√Ω t·ª± ng·∫´u nhi√™n
            words[i] = random.choice('abcdefghijklmnopqrstuvwxyz')
    return ''.join(words)  # Gh√©p l·∫°i th√†nh chu·ªói


def random_word_deletion(text, probability=0.2):
    words = text.split()
    new_words = [word for word in words if random.random() > probability]
    if len(new_words) == 0:  # Ensure at least one word remains
        return random.choice(words)
    return ' '.join(new_words)


async def back_translation(text, lang='fr'):

    translator = Translator()
    translated = await translator.translate(text, dest=lang)
    back_translated = await translator.translate(translated.text, src=lang, dest=detect(text))
    return back_translated.text


def tokenize(corpus):
    words = [word.lower() for sentence in corpus for word in sentence.split()]
    vocab = list(set(words))
    word_to_ix = {word: i for i, word in enumerate(vocab)}
    ix_to_word = {i: word for word, i in word_to_ix.items()}
    return words, vocab, word_to_ix, ix_to_word


def create_cbows(words, word_to_ix, window_size=2):
    data = []
    for i in range(window_size, len(words) - window_size):
        context = [words[i - j] for j in range(1, window_size + 1)] + \
            [words[i + j] for j in range(1, window_size + 1)]
        target = words[i]
        data.append((context, target))
    return data


def create_skipgrams(words, word_to_ix, window_size=2):
    data = []
    for i in range(window_size, len(words) - window_size):
        target = words[i]
        context = [words[i - j] for j in range(1, window_size + 1)] + \
                  [words[i + j] for j in range(1, window_size + 1)]
        for ctx in context:
            # (target, context)
            data.append((word_to_ix[target], word_to_ix[ctx]))
    return data


def build_vocab(corpus):
    words = [word.lower() for sentence in corpus for word in sentence.split()]
    vocab = list(set(words))
    word_to_ix = {word: i for i, word in enumerate(vocab)}
    ix_to_word = {i: word for word, i in word_to_ix.items()}
    return vocab, word_to_ix, ix_to_word


def build_cooccurrence_matrix(corpus, word_to_ix, window_size=2):
    matrix = np.zeros((vocab_size, vocab_size))

    for sentence in corpus:
        words = sentence.lower().split()
        for i, word in enumerate(words):
            word_idx = word_to_ix[word]
            for j in range(1, window_size + 1):
                if i - j >= 0:
                    context_idx = word_to_ix[words[i - j]]
                    matrix[word_idx, context_idx] += 1
                if i + j < len(words):
                    context_idx = word_to_ix[words[i + j]]
                    matrix[word_idx, context_idx] += 1

    return matrix


class GloVe:
    def __init__(self, vocab_size, embedding_dim=10, alpha=0.75, x_max=100):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.alpha = alpha
        self.x_max = x_max

        # Kh·ªüi t·∫°o vector t·ª´ v√† bias
        self.W = np.random.rand(vocab_size, embedding_dim)
        self.W_tilde = np.random.rand(vocab_size, embedding_dim)
        self.b = np.zeros(vocab_size)
        self.b_tilde = np.zeros(vocab_size)

    def weight_function(self, x):
        return (x / self.x_max) ** self.alpha if x < self.x_max else 1

    def train(self, cooccurrence_matrix, epochs=100, learning_rate=0.05):
        nonzero_indices = np.nonzero(cooccurrence_matrix)

        for epoch in range(epochs):
            total_loss = 0
            for i, j in zip(*nonzero_indices):
                X_ij = cooccurrence_matrix[i, j]
                weight = self.weight_function(X_ij)

                # T√≠nh to√°n l·ªói loss
                loss_ij = (self.W[i].dot(self.W_tilde[j]) +
                           self.b[i] + self.b_tilde[j] - np.log(X_ij)) ** 2
                total_loss += weight * loss_ij

                # C·∫≠p nh·∫≠t vector t·ª´ v√† bias
                gradient = 2 * weight * \
                    (self.W[i].dot(self.W_tilde[j]) +
                     self.b[i] + self.b_tilde[j] - np.log(X_ij))

                self.W[i] -= learning_rate * gradient * self.W_tilde[j]
                self.W_tilde[j] -= learning_rate * gradient * self.W[i]
                self.b[i] -= learning_rate * gradient
                self.b_tilde[j] -= learning_rate * gradient

            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {total_loss:.4f}")


if "step" not in st.session_state:
    st.session_state.step = 1


progress = st.progress(st.session_state.step)

step2 = sac.steps(
    items=[
        sac.StepsItem(title='B∆∞·ªõc 1',  description='Thu th·∫≠p'),
        sac.StepsItem(title='B∆∞·ªõc 2', description='TƒÉng c∆∞·ªùng'),
        sac.StepsItem(title='B∆∞·ªõc 3', description='Ti·ªÅn x·ª≠ l√Ω'),
        sac.StepsItem(title='B∆∞·ªõc 4', description='Bi·ªÉu di·ªÖn'),
        sac.StepsItem(title='B∆∞·ªõc 5', description='Ph√¢n lo·∫°i'),
    ], color='green', return_index=True
)
st.session_state.step = step2+1
progress.progress(st.session_state.step/5)
if st.session_state.step == 1:
    nltk.download('punkt_tab')
    nltk.download('stopwords')
    nltk.download('averaged_perceptron_tagger_eng')
    nltk.download('wordnet')
    st.header("B∆∞·ªõc 1 Thu th·∫≠p d·ªØ li·ªáu ")
    if "text" not in st.session_state:
        st.session_state.text = []


    
    option = st.radio("Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p vƒÉn b·∫£n:", [
        "Nh·∫≠p s·ªë ƒë·ªÉ l·∫•y vƒÉn b·∫£n",
        "T·∫£i l√™n file .txt",
        "Nh·∫≠p t·ª´ file json",
        "Nh·∫≠p t·ª´ file csv",
    ])
    if option == "T·∫£i l√™n file .txt":
        uploaded_file = st.file_uploader("Ch·ªçn t·ªáp ƒë·ªÉ t·∫£i l√™n", type=["txt"])
        if uploaded_file is not None:
            stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
            text = stringio.read()
            # st.session_state.text = textVanw
            data = {'title': ["VƒÉn b·∫£n c·ªßa ng∆∞·ªùi d√πng"],
                    'ups': [0], 'text': text, }
            st.session_state.df = pd.DataFrame.from_dict(data)
            # st.text_area("N·ªôi dung t·ªáp:", text, height=150)
    elif option == "Nh·∫≠p s·ªë ƒë·ªÉ l·∫•y vƒÉn b·∫£n":
        col1, col2 = st.columns(2)
        option_sub = col1.selectbox("Ch·ªçn subredit ƒë·ªÉ crawl", [
            "shortscarystories",
            "nosleep",
            "creepypasta",
            "news",]
        )
        dessub = ""
        
        if option_sub == "shortscarystories":
            dessub = "Truy·ªán ng·∫Øn kinh d·ªã - Ng√¥i nh√† c·ªßa truy·ªán kinh d·ªã Flash Fiction. Ch√∫ng t√¥i mang ƒë·∫øn nh·ªØng c√¢u chuy·ªán kinh d·ªã, h·ªìi h·ªôp v√† nh·ªØng t√¨nh ti·∫øt ƒëau l√≤ng trong 500 t·ª´ ho·∫∑c √≠t h∆°n"
        elif option_sub == "creepypasta":
            dessub = "N∆°i d√†nh cho nh·ªØng ng∆∞·ªùi h√¢m m·ªô truy·ªán Creepypasta."
        elif option_sub == "nosleep":
            dessub = "Nosleep l√† n∆°i ƒë·ªÉ ng∆∞·ªùi d√πng Reddit chia s·∫ª nh·ªØng tr·∫£i nghi·ªám c√° nh√¢n ƒë√°ng s·ª£ c·ªßa h·ªç."
        elif option_sub == "news":
            dessub = "N∆°i ƒëƒÉng t·∫£i c√°c b√†i vi·∫øt v·ªÅ c√°c s·ª± ki·ªán hi·ªán t·∫°i ·ªü Hoa K·ª≥ v√† ph·∫ßn c√≤n l·∫°i c·ªßa th·∫ø gi·ªõi. Th·∫£o lu·∫≠n t·∫•t c·∫£ ·ªü ƒë√¢y."
        col2.write(dessub)
        number = st.number_input("Nh·∫≠p m·ªôt s·ªë:", min_value=1,
                                 step=1, max_value=50, value=3)
        if st.button("Crawl"):
            df = crawl_text(number, option_sub)
            st.dataframe(df)
            st.session_state.df = df
            # st.text_area("VƒÉn b·∫£n ƒë√£ crawl:", str(text), height=150)
    elif option == "Nh·∫≠p t·ª´ file json":
        uploaded_file = st.file_uploader("Ch·ªçn t·ªáp ƒë·ªÉ t·∫£i l√™n", type=["json"])
        if uploaded_file is not None:
            # ƒê·ªçc v√† parse file JSON
            file_content = uploaded_file.read().decode("utf-8")
            try:
                json_data = json.loads(file_content)

                # Chuy·ªÉn JSON th√†nh DataFrame
                st.session_state.df = pd.DataFrame(json_data)

                st.success("T·∫£i d·ªØ li·ªáu th√†nh c√¥ng!")
                st.dataframe(st.session_state.df)
            except json.JSONDecodeError:
                st.error("T·ªáp kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng JSON.")
    elif option == "Nh·∫≠p t·ª´ file csv":
        uploaded_file = st.file_uploader("Ch·ªçn t·ªáp ƒë·ªÉ t·∫£i l√™n", type=["csv"])
        if uploaded_file is not None:
            # ƒê·ªçc v√† parse file CSV
            file_content = uploaded_file.read().decode("utf-8")
            try:
                csv_data = pd.read_csv(StringIO(file_content))

                # Chuy·ªÉn CSV th√†nh DataFrame
                st.session_state.df = csv_data

                st.success("T·∫£i d·ªØ li·ªáu th√†nh c√¥ng!")
                st.dataframe(st.session_state.df)
            except pd.errors.ParserError:
                st.error("T·ªáp kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng CSV.")
            
    

elif st.session_state.step == 2:
    st.header("B∆∞·ªõc 2 TƒÉng c∆∞·ªùng d·ªØ li·ªáu")

    b2op = st.radio("Ch·ªçn ph∆∞∆°ng ph√°p tƒÉng c∆∞·ªùng", ["Thay th·∫ø t·ª´ ƒë·ªìng nghƒ©a",
                                                    "X√°o tr·ªôn t·ª´",
                                                    "Th√™m nhi·ªÖu",
                                                    "X√≥a t·ª´ ng·∫´u nhi√™n",
                                                    "D·ªãch ng∆∞·ª£c "], horizontal=True)
    num_row = st.number_input("Nh·∫≠p s·ªë b√†i vi·∫øt c·∫ßn tƒÉng c∆∞·ªùng", min_value=4)

    if b2op == "Thay th·∫ø t·ª´ ƒë·ªìng nghƒ©a":
        st.write("B·∫°n ƒë√£ ch·ªçn ph∆∞∆°ng ph√°p thay th·∫ø t·ª´ ƒë·ªìng nghƒ©a.")
        # synonym_replacement_EN(st.session_state.df)
        num_dongnghia = st.number_input(
            "Nh·∫≠p s·ªë t·ª´ thay th·∫ø tr√™n m·ªói b√†i", min_value=4)

    elif b2op == "X√°o tr·ªôn t·ª´":
        st.write("B·∫°n ƒë√£ ch·ªçn ph∆∞∆°ng ph√°p x√°o tr·ªôn t·ª´.")
        # Th√™m logic x·ª≠ l√Ω t·∫°i ƒë√¢y

    elif b2op == "Th√™m nhi·ªÖu":
        st.write("B·∫°n ƒë√£ ch·ªçn ph∆∞∆°ng ph√°p th√™m nhi·ªÖu.")
        num_xacsuat = st.number_input(
            label="Nh·∫≠p x√°c su·∫•t nhi·ªÖu", min_value=0.1, max_value=1.00, step=0.05)

    elif b2op == "X√≥a t·ª´ ng·∫´u nhi√™n":
        st.write("B·∫°n ƒë√£ ch·ªçn ph∆∞∆°ng ph√°p x√≥a t·ª´ ng·∫´u nhi√™n.")
        # Th√™m logic x·ª≠ l√Ω t·∫°i ƒë√¢y
        num_xacsuat = st.number_input(
            label="Nh·∫≠p x√°c su·∫•t nhi·ªÖu", min_value=0.1, max_value=1.00, step=0.05)
    elif b2op == "D·ªãch ng∆∞·ª£c":
        st.write("B·∫°n ƒë√£ ch·ªçn ph∆∞∆°ng ph√°p d·ªãch ng∆∞·ª£c.")
        # Th√™m logic x·ª≠ l√Ω t·∫°i ƒë√¢y

    b2btn = st.button("TƒÉng c∆∞·ªùng")
    if b2btn:
        if "df" not in st.session_state:
            st.error("Vui l√≤ng crawl d·ªØ li·ªáu tr∆∞·ªõc khi tƒÉng c∆∞·ªùng!")
        else:
            new_rows = []
            for i in range(num_row):
                random_num = random.randint(0, len(st.session_state.df) - 1)
                row = st.session_state.df.loc[random_num].copy()
                if b2op == "Thay th·∫ø t·ª´ ƒë·ªìng nghƒ©a":
                    row['text'] = synonym_replacement_EN(
                        row['text'], num_dongnghia)
                elif b2op == "X√°o tr·ªôn t·ª´":
                    row['text'] = word_shuffling(row['text'])
                elif b2op == "Th√™m nhi·ªÖu":
                    row['text'] = noise_injection(row['text'], num_xacsuat)
                elif b2op == "X√≥a t·ª´ ng·∫´u nhi√™n":
                    row['text'] = random_word_deletion(
                        row['text'], num_xacsuat)
                elif b2op == "D·ªãch ng∆∞·ª£c":
                    row['text'] = translate_and_back(row['text'], num_row)
                new_rows.append(row)
            st.session_state.df = pd.concat(
                [st.session_state.df, pd.DataFrame(new_rows)], ignore_index=True
            )
            st.dataframe(st.session_state.df)
    if "df" in st.session_state:
        pass
        # st.dataframe(st.session_state.df)
        # st.write("So luong entry: ",len(st.session_state.df))

elif st.session_state.step == 3:
    st.header("B∆∞·ªõc 3 X·ª≠ l√Ω ti·ªÅn d·ªØ li·ªáu")
    nltk.download('punkt_tab')
    nltk.download('stopwords')
    nltk.download('averaged_perceptron_tagger_eng')
    nltk.download('wordnet')
    nlp = spacy.load("en_core_web_sm")
    col1, col2, col3 = st.columns(3)

    with col1:
        clean_lower = st.toggle("Chuy·ªÉn ƒë·ªïi ch·ªØ th∆∞·ªùng")
        clean_numbers = st.toggle("X√≥a s·ªë")
        clean_symbols = st.toggle("X√≥a c√°c bi·ªÉu t∆∞·ª£ng")
        clean_ExpandingContractions = st.toggle("X·ª≠ l√Ω t·ª´ vi·∫øt t·∫Øt")

    with col2:
        clean_PunctuationRemoval = st.toggle("Lo·∫°i b·ªè d·∫•u c√¢u")
        clean_spaces = st.toggle("Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a")
        clean_specialchar = st.toggle("Nh·∫≠n di·ªán v√† lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát")

    with col3:
        clean_StopWordsRemoval = st.toggle("X√≥a t·ª´ d·ª´ng")
        clean_Stemming = st.toggle("C·∫Øt g·ªëc t·ª´ Stemming")
        clean_Lemmatization = st.toggle(
            "Chu·∫©n h√≥a t·ª´ v·ªÅ t·ª´ ƒëi·ªÉn Lemmatization")

    if st.button("X·ª≠ l√Ω"):
        st.session_state.df2 = st.session_state.df.copy()
        st.session_state.df2["text"] = st.session_state.df2["text"].apply(
            clean_text)
        st.dataframe(st.session_state.df2)
    if "df2" in st.session_state:
        # st.dataframe(st.session_state.df2)
        st.write("So luong entry: ", len(st.session_state.df2))

    option_3_2 = st.radio("Ch·ªçn c√°ch x·ª≠ l√Ω vƒÉn b·∫£n:", ["NER ƒë·ªÉ hi·ªÉn th·ªã th·ª±c th·ªÉ",
                                                       "Tokenization",
                                                       "POS tagging",
                                                       "Parsing"], horizontal=True)
    if option_3_2 == "Parsing" or \
            option_3_2 == "NER ƒë·ªÉ hi·ªÉn th·ªã th·ª±c th·ªÉ"\
            and "df2" in st.session_state:
        selected_text = st.selectbox(
            "Ch·ªçn b√†i x·ª≠ l√Ω:", st.session_state.df2["text"])

        # Ph√¢n t√≠ch c√∫ ph√°p v·ªõi spaCy
        if str(selected_text) != "":
            doc = nlp(str(selected_text))
            sentences = list(doc.sents)
            selected_sent = st.selectbox(
                "Ch·ªçn c√¢u x·ª≠ l√Ω", [str(sent) for sent in sentences])
    if st.button("xu ly 3.2"):
        st.session_state.df3 = st.session_state.df2.copy()
        if option_3_2 == "NER ƒë·ªÉ hi·ªÉn th·ªã th·ª±c th·ªÉ":
            doc_sent = nlp(str(selected_sent))

            # 1Ô∏è‚É£ T·∫°o danh s√°ch th·ª±c th·ªÉ
            entities = [{"Th·ª±c th·ªÉ": ent.text, "Lo·∫°i": ent.label_}
                        for ent in doc_sent.ents]

            # 2Ô∏è‚É£ Chuy·ªÉn th√†nh DataFrame
            df_entities = pd.DataFrame(entities)
            if not df_entities.empty:
                st.table(df_entities)
            else:
                st.write("Kh√¥ng t√¨m th·∫•y th·ª±c th·ªÉ n√†o!")

        elif option_3_2 == "Tokenization":
            # st.title("T·∫°o Word Cloud t·ª´ DataFrame")

            # Hi·ªÉn th·ªã DataFrame
            st.subheader("D·ªØ li·ªáu vƒÉn b·∫£n:")
            st.write(st.session_state.df2)

            # 1Ô∏è‚É£ G·ªôp to√†n b·ªô vƒÉn b·∫£n trong c·ªôt "text" th√†nh m·ªôt chu·ªói
            # Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
            text_data = " ".join(st.session_state.df2["text"]).lower()

            # 2Ô∏è‚É£ Tokenization
            tokens = word_tokenize(text_data)

            # 3Ô∏è‚É£ X√≥a d·∫•u c√¢u
            tokens = [word for word in tokens if word not in string.punctuation]

            # 4Ô∏è‚É£ G·ªôp danh s√°ch tokens th√†nh chu·ªói
            processed_text = " ".join(tokens)

            # 5Ô∏è‚É£ In k·∫øt qu·∫£ ki·ªÉm tra
            # st.write("üìå VƒÉn b·∫£n sau khi x·ª≠ l√Ω:")
            # st.write(processed_text)

            # 6Ô∏è‚É£ T·∫°o & hi·ªÉn th·ªã Word Cloud
            wordcloud = WordCloud(
                width=800, height=400, background_color="white").generate(processed_text)

            st.subheader("Word Cloud")
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.imshow(wordcloud, interpolation="bilinear")
            ax.axis("off")
            st.pyplot(fig)

        elif option_3_2 == "POS tagging":
            text_data = " ".join(st.session_state.df3["text"]).lower()

            # 2Ô∏è‚É£ Tokenization
            tokens = word_tokenize(text_data)

            # 3Ô∏è‚É£ POS Tagging
            pos_tags = nltk.pos_tag(tokens)

            # 4Ô∏è‚É£ ƒê·∫øm s·ªë l∆∞·ª£ng t·ª´ng lo·∫°i t·ª´
            pos_counts = Counter(tag for _, tag in pos_tags)

            # 5Ô∏è‚É£ Chuy·ªÉn d·ªØ li·ªáu th√†nh DataFrame ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
            df_pos = pd.DataFrame(pos_counts.items(), columns=[
                                  "POS Tag", "Count"])
            df_pos = df_pos.sort_values(
                by="Count", ascending=False)  # S·∫Øp x·∫øp gi·∫£m d·∫ßn

            # 6Ô∏è‚É£ V·∫Ω Bar Chart v·ªõi Seaborn
            st.subheader("Bar Chart - Ph√¢n lo·∫°i t·ª´ lo·∫°i (POS)")
            fig, ax = plt.subplots(figsize=(10, 5))
            sns.barplot(x=df_pos["POS Tag"],
                        y=df_pos["Count"], palette="viridis", ax=ax)
            ax.set_xlabel("POS Tag")
            ax.set_ylabel("S·ªë l∆∞·ª£ng")
            ax.set_title("T·∫ßn su·∫•t c√°c t·ª´ lo·∫°i trong vƒÉn b·∫£n")
            ax.set_xticklabels(ax.get_xticklabels(), rotation=90)

            st.pyplot(fig)

            # # 7Ô∏è‚É£ Hi·ªÉn th·ªã d·ªØ li·ªáu POS ƒë·ªÉ ki·ªÉm tra
            # st.subheader("Chi ti·∫øt POS Tagging")
            # st.write(pos_tags)

        elif option_3_2 == "Parsing":

            doc_sent = nlp(str(selected_sent))
            # 1Ô∏è‚É£ T·∫°o ƒë·ªì th·ªã
            G = nx.DiGraph()

            # 2Ô∏è‚É£ Th√™m c√°c t·ª´ v√†o ƒë·ªì th·ªã
            for token in doc_sent:
                G.add_edge(token.head.text, token.text, label=token.dep_)

            # 3Ô∏è‚É£ V·∫Ω ƒë·ªì th·ªã
            st.subheader("Graph Network - C√¢y C√∫ Ph√°p")

            fig, ax = plt.subplots(figsize=(10, 10))
            pos = nx.spring_layout(G)  # B·ªë c·ª•c c·ªßa ƒë·ªì th·ªã
            nx.draw(G, pos, with_labels=True, node_color="lightblue", edge_color="gray",
                    node_size=2000, font_size=10, font_weight="bold", ax=ax)

            # Hi·ªÉn th·ªã nh√£n dependency tr√™n c√°c c·∫°nh
            edge_labels = {(token.head.text, token.text): token.dep_ for token in doc_sent}
            nx.draw_networkx_edge_labels(
                G, pos, edge_labels=edge_labels, font_color="red", ax=ax)

            # Hi·ªÉn th·ªã ƒë·ªì th·ªã tr√™n Streamlit
            st.pyplot(fig)

            # Hi·ªÉn th·ªã chi ti·∫øt dependency tree
            st.subheader("Chi ti·∫øt Dependency Parsing:")
            dic = {'T·ª´ hi·ªán t·∫°i trong c√¢u': [token.text for token in doc_sent],
                   'Quan h·ªá c√∫ ph√°p': [token.dep_ for token in doc_sent],
                   'T·ª´ g·ªëc': [token.head.text for token in doc_sent],
                   }
            st.dataframe(pd.DataFrame.from_dict(dic))
            # for token in doc_sent:
            #     st.write(f"**{token.text}** ‚Üê ({token.dep_}) ‚Üê **{token.head.text}**")

elif st.session_state.step == 4:

    nlp = spacy.load("en_core_web_sm")

    st.header("B∆∞·ªõc 4 Text Representation")
    # st.title("B∆∞·ªõc 5 Text Distributed Representations")
    option_pre = st.radio("Ch·ªçn c√°ch x·ª≠ l√Ω vƒÉn b·∫£n:",
                          [
                              "One-Hot Encoding",
                              "Bag of Words (BoW)",
                              "Bag of N-Grams",
                              "TF-IDF",
                              "CBOW",
                              "skip gram",
                              "GloVe",
                              # "ELMo",
                              "BERT"
                          ], horizontal=True)

    if "df2" in st.session_state:

        if option_pre == "One-Hot Encoding" or option_pre == "Bag of Words (BoW)" or option_pre == "TF-IDF":
            selected_text = st.selectbox(
                "Ch·ªçn b√†i ƒë·ªÉ ph√¢n t√≠ch:", st.session_state.df2["text"])
            if str(selected_text) != "":
                doc = nlp(str(selected_text))
                sentences = list(doc.sents)
        elif option_pre == "Bag of N-Grams":
            selected_text = st.selectbox(
                "Ch·ªçn b√†i ƒë·ªÉ ph√¢n t√≠ch:", st.session_state.df2["text"])
            if str(selected_text) != "":
                doc = nlp(str(selected_text))
                sentences = list(doc.sents)
            start_gram = st.number_input("start_gram", value=2)
            end_gram = st.number_input("end_gram", value=2)
        elif option_pre == "CBOW":
            selected_text = st.selectbox(
                "Ch·ªçn b√†i ƒë·ªÉ ph√¢n t√≠ch CBOW:", st.session_state.df2["text"])
            if str(selected_text) != "":
                doc = nlp(str(selected_text))
                sentences = list(doc.sents)
                # selected_sent=st.selectbox("Ch·ªçn c√¢u x·ª≠ l√Ω CBOW",[str(sent) for sent in sentences])
                corpus = [str(sent) for sent in sentences]
                words, vocab, word_to_ix, ix_to_word = tokenize(corpus)
                a = list(word_to_ix.keys())
                a.sort()
                testword = st.selectbox("Ch·ªçn t·ª´ ƒë·ªÉ ki·ªÉm tra", a)
        elif option_pre == "skip gram":
            selected_text = st.selectbox(
                "Ch·ªçn b√†i ƒë·ªÉ ph√¢n t√≠ch skip gram:", st.session_state.df2["text"])
            if str(selected_text) != "":
                doc = nlp(str(selected_text))
                sentences = list(doc.sents)
                # selected_sent=st.selectbox("Ch·ªçn c√¢u x·ª≠ l√Ω CBOW",[str(sent) for sent in sentences])
                corpus = [str(sent) for sent in sentences]
                words, vocab, word_to_ix, ix_to_word = tokenize(corpus)
                a = list(word_to_ix.keys())
                a.sort()
                testword = st.selectbox("Ch·ªçn t·ª´ ƒë·ªÉ ki·ªÉm tra", a)
        elif option_pre == "GloVe":
            selected_text = st.selectbox(
                "Ch·ªçn b√†i ƒë·ªÉ ph√¢n t√≠ch GloVe:", st.session_state.df2["text"])
            if str(selected_text) != "":
                doc = nlp(str(selected_text))
                sentences = list(doc.sents)
                # selected_sent=st.selectbox("Ch·ªçn c√¢u x·ª≠ l√Ω CBOW",[str(sent) for sent in sentences])
                corpus = [str(sent) for sent in sentences]
                vocab, word_to_ix, ix_to_word = build_vocab(corpus)
                vocab_size = len(vocab)
                a = list(word_to_ix.keys())
                a.sort()
                testword = st.selectbox("Ch·ªçn t·ª´ ƒë·ªÉ ki·ªÉm tra", a)
        elif option_pre == "BERT":
            selected_text = st.selectbox(
                "Ch·ªçn b√†i ƒë·ªÉ ph√¢n t√≠ch BERT:", st.session_state.df2["text"])
            if str(selected_text) != "":
                doc = nlp(str(selected_text))
                sentences = list(doc.sents)
        # elif option_pre=="ELMo":
        #     selected_text = st.selectbox("Ch·ªçn b√†i ƒë·ªÉ ph√¢n t√≠ch ELMo:", st.session_state.df2["text"])
        #     if  str(selected_text) !="" :
        #         doc = nlp(str(selected_text))
        #         sentences = list(doc.sents)

    if st.button("X·ª≠ l√Ω bi·ªÉu di·ªÖn"):
        st.session_state.df3 = st.session_state.df2.copy()

        if option_pre == "One-Hot Encoding":
            count_vect = CountVectorizer(binary=True)
            bow_rep = count_vect.fit_transform(
                [str(span) for span in sentences])
            # print (bow_rep.toarray())
            # print(count_vect.get_feature_names_out())
            df = pd.DataFrame(bow_rep.toarray(), columns=count_vect.get_feature_names_out(),
                              #   index=[str(span) for span in sentences]
                              )
            st.dataframe(df)

        elif option_pre == "Bag of Words (BoW)":
            count_vect = CountVectorizer()
            bow_rep = count_vect.fit_transform(
                [str(span) for span in sentences])

            df3 = pd.DataFrame(bow_rep.toarray(),
                               columns=count_vect.get_feature_names_out(),)
            st.dataframe(df3)
        elif option_pre == "Bag of N-Grams":
            count_vect = CountVectorizer(ngram_range=(start_gram, end_gram))
            bow_rep = count_vect.fit_transform(
                [str(span) for span in sentences])
            df3 = pd.DataFrame(bow_rep.toarray(),
                               columns=count_vect.get_feature_names_out(), )
            st.dataframe(df3)
        elif option_pre == "TF-IDF":
            count_vect = TfidfVectorizer()
            bow_rep = count_vect.fit_transform(
                [str(span) for span in sentences])

            df3 = pd.DataFrame(bow_rep.toarray(),
                               columns=count_vect.get_feature_names_out(),)
            st.dataframe(df3)
        elif option_pre == "CBOW":

            # T·∫°o d·ªØ li·ªáu cho CBOW (window_size = 2)

            cbow_data = create_cbows(words, word_to_ix)

            # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh ch·ªâ s·ªë
            def prepare_data(cbow_data, word_to_ix):
                inputs, targets = [], []
                for context, target in cbow_data:
                    inputs.append([word_to_ix[word] for word in context])
                    targets.append(word_to_ix[target])
                return torch.tensor(inputs), torch.tensor(targets)

            inputs, targets = prepare_data(cbow_data, word_to_ix)

            # X√¢y d·ª±ng m√¥ h√¨nh CBOW
            class CBOWModel(nn.Module):
                def __init__(self, vocab_size, embed_dim):
                    super(CBOWModel, self).__init__()
                    self.embeddings = nn.Embedding(vocab_size, embed_dim)
                    self.linear = nn.Linear(embed_dim, vocab_size)

                def forward(self, context):
                    embeds = self.embeddings(context).mean(
                        dim=1)  # T√≠nh trung b√¨nh embedding
                    out = self.linear(embeds)
                    return out

            # Kh·ªüi t·∫°o m√¥ h√¨nh
            embed_dim = 10
            model = CBOWModel(len(vocab), embed_dim)
            loss_function = nn.CrossEntropyLoss()
            optimizer = optim.Adam(model.parameters(), lr=0.01)

            # Hu·∫•n luy·ªán m√¥ h√¨nh
            num_epochs = 100
            for epoch in range(num_epochs):
                optimizer.zero_grad()
                output = model(inputs)
                loss = loss_function(output, targets)
                loss.backward()
                optimizer.step()
                if epoch % 10 == 0:
                    continue
                    print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

            # Ki·ªÉm tra embedding c·ªßa m·ªôt t·ª´

            word_idx = word_to_ix[testword]
            st.write(
                f"Embedding c·ªßa '{testword}': {model.embeddings.weight[word_idx].detach().numpy()}")
            # print(f"Embedding c·ªßa 'nlp': {model.embeddings.weight[word_idx].detach().numpy()}")

        elif option_pre == "skip gram":
            skipgram_data = create_skipgrams(words, word_to_ix)
            # skipgram_data = create_skipgrams(words, word_to_ix)

            # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh tensor
            def prepare_data(skipgram_data):
                inputs, targets = zip(*skipgram_data)
                return torch.tensor(inputs), torch.tensor(targets)

            inputs, targets = prepare_data(skipgram_data)

            # X√¢y d·ª±ng m√¥ h√¨nh Skip-gram
            class SkipGramModel(nn.Module):
                def __init__(self, vocab_size, embed_dim):
                    super(SkipGramModel, self).__init__()
                    self.embeddings = nn.Embedding(vocab_size, embed_dim)
                    self.linear = nn.Linear(embed_dim, vocab_size)

                def forward(self, target):
                    embed = self.embeddings(target)
                    out = self.linear(embed)
                    return out

            # Kh·ªüi t·∫°o m√¥ h√¨nh
            embed_dim = 10
            model = SkipGramModel(len(vocab), embed_dim)
            loss_function = nn.CrossEntropyLoss()
            optimizer = optim.Adam(model.parameters(), lr=0.01)

            # Hu·∫•n luy·ªán m√¥ h√¨nh
            num_epochs = 100
            for epoch in range(num_epochs):
                optimizer.zero_grad()
                output = model(inputs)
                loss = loss_function(output, targets)
                loss.backward()
                optimizer.step()
                if epoch % 10 == 0:
                    print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
            # result = st.text_area(label="ket qua", value=t)
            word_idx = word_to_ix[testword]
            st.write(
                f"Embedding c·ªßa '{testword}': {model.embeddings.weight[word_idx].detach().numpy()}")
        elif option_pre == "GloVe":
            cooccurrence_matrix = build_cooccurrence_matrix(corpus, word_to_ix)
            glove_model = GloVe(vocab_size, embedding_dim=10)
            glove_model.train(cooccurrence_matrix, epochs=100)
            word_idx = word_to_ix[testword]
            st.write(f"Embedding c·ªßa '{testword}': {glove_model.W[word_idx]}")
        elif option_pre == "BERT":
            model_name = "bert-base-uncased"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            bow_rep = tokenizer([str(span) for span in sentences],
                                padding=True, truncation=True, return_tensors="pt")
            df3 = pd.DataFrame(bow_rep["input_ids"].numpy(), index=sentences)
            st.dataframe(df3)
        elif option_pre == "ELMo":
            pass
            # elmo.signatures["default"](tf.constant([str(span) for span in sentences]))["elmo"]
            # df

elif st.session_state.step == 5:
    nltk.download('punkt_tab')
    nltk.download('stopwords')
    nltk.download('averaged_perceptron_tagger_eng')
    nltk.download('wordnet')
    nlp = spacy.load("en_core_web_sm")
    st.header("B∆∞·ªõc 5: Text Classification")

    option_mission = st.selectbox("Ch·ªçn nhi·ªám v·ª•:",
                                  ["setiment analysis",
                                   "topic classification",
                                   "question classification",
                                   ])
    if option_mission == "setiment analysis":
        option_dataset = st.selectbox("Ch·ªçn t·∫≠p d·ªØ li·ªáu:", [
            "SST-2",
            "IMDb",
            "Yelp"])
        if option_dataset == "IMDb":
            # st.image("imdb_result.png")
            pass
        elif option_dataset == "SST-2":
            # st.image("sst2_result.png")
            pass
        elif option_dataset == "Yelp":
            pass

    if option_mission == "topic classification":
        option_dataset = st.selectbox("Ch·ªçn t·∫≠p d·ªØ li·ªáu:", [
            "ag_news",
            # "dbpedia",
            # "yahoo_answers",
            # "yelp_review_full",
            # "yelp_review_polarity"
        ])
        # st.image("ag_news_result.png")

    option_encode = st.selectbox("Ch·ªçn ph∆∞∆°ng ph√°p vector h√≥a:", [
        "TF-IDF",
        "Bag of Words",
        "One-Hot Encoding",
        "Bag of n-grams",

        #    "Word2Vec",
        #    "BERT"
    ])

    option_model = st.multiselect("Ch·ªçn m√¥ h√¨nh ph√¢n lo·∫°i:", ["Logistic Regression", "SVM",
                                                              #   "Random Forest  ",
                                                              "Naive Bayes",
                                                              "knn",
                                                              "Decision Tree",
                                                              #   "Maximizing Likelihood",
                                                              # "BERT"
                                                              ], default=["Logistic Regression"])
    if option_model == "Decision Tree":
        # st.image("DecisionTree_result.png")
        pass
    percent_test_size = st.number_input(
        "Nh·∫≠p t·ª∑ l·ªá d·ªØ li·ªáu test:", min_value=0.1, max_value=0.9, step=0.1, value=0.3)
    #
    # st.text_area("Nh·∫≠p vƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i:")

    # example=st.text_area("Nh·∫≠p vƒÉn ph√¢n lo·∫°i")
    #     st.selectbox("Ch·ªçn b√†i vi·∫øt ƒë·ªÉ ph√¢n lo·∫°i",st.session_state.df3["text"])
    if option_model == "Logistic Regression":
        pass
    listmodel = []
    if st.button("Ph√¢n lo·∫°i tr√™n d·ªØ li·ªáu "):

        if "Logistic Regression" in option_model:
            listmodel.append(LogisticRegression(max_iter=1000))
        if "SVM" in option_model:
            listmodel.append(SVC())

        if "Naive Bayes" in option_model:
            listmodel.append(MultinomialNB())

        if "knn" in option_model:
            listmodel.append(KNeighborsClassifier(n_neighbors=5))
        if "Decision Tree" in option_model:
            listmodel.append(DecisionTreeClassifier())

        if option_encode == "Bag of N Words":
            model_encode = CountVectorizer()
        elif option_encode == "TF-IDF":
            model_encode = TfidfVectorizer()
        elif option_encode == "Bag of N Words":
            model_encode = CountVectorizer(ngram_range=(1, 3))
        elif option_encode == "One-Hot Encoding":
            model_encode = CountVectorizer(binary=True)

        # elif option_encode == "BERT":

        #     model_encode = BertTokenizer.from_pretrained("bert-base-uncased")
        dataset=None
        if option_mission == "setiment analysis":
            if option_dataset == "IMDb":
                dataset = load_dataset("imdb")
                train_texts = dataset["train"]["text"]
                train_labels = dataset["train"]["label"]
                test_texts = dataset["test"]["text"]
                test_labels = dataset["test"]["label"]
            elif option_dataset == "SST-2":
                dataset = load_dataset("glue", "sst2")
                train_texts = dataset["train"]["sentence"]
                train_labels = dataset["train"]["label"]
                test_texts = dataset["validation"]["sentence"]
                test_labels = dataset["validation"]["label"]
                st.write("load data complete")
            elif option_dataset == "Yelp":
                dataset = load_dataset("yelp_polarity")
                train_texts = dataset["train"]["text"]
                train_labels = dataset["train"]["label"]
                test_texts = dataset["test"]["text"]
                test_labels = dataset["test"]["label"]
        elif option_mission == "topic classification":
            dataset = load_dataset("ag_news")
            print(dataset)

            train_texts = dataset["train"]["text"]
            train_labels = dataset["train"]["label"]
            test_texts = dataset["test"]["text"]
            test_labels = dataset["test"]["label"]
        elif option_mission == "question classification":
            dataset = load_dataset("CogComp/trec")
            train_texts = dataset["train"]["text"]
            train_labels = dataset["train"]["coarse_label"]
            test_texts = dataset["test"]["text"]
            test_labels = dataset["test"]["coarse_label"]
        # st.write("load data complete")
        all_texts = train_texts+test_texts
        all_labels = train_labels+test_labels
        train_texts, test_texts, train_labels, test_labels = train_test_split(
            all_texts, all_labels, test_size=percent_test_size, random_state=42)
        list_result = []
        list_pipline = []
        with st.spinner("ƒêang kh·ªüi t·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh b·∫±ng m√¥ h√¨nh ƒë√£ ch·ªçn...", show_time=True):
            for model in listmodel:

                pipline_cls = make_pipeline(model_encode, model)
                st.write("Kh·ªüi t·∫°o m√¥ h√¨nh", str(pipline_cls))
                pipline_cls.fit(train_texts, train_labels)
                list_result.append(pipline_cls.score(test_texts, test_labels))
                list_pipline.append(pipline_cls)
                st.write(str(pipline_cls), "k·∫øt qu·∫£ tr√™n t·∫≠p test: ",
                         pipline_cls.score(test_texts, test_labels))
        st.session_state.list_pipline = list_pipline
        # st.session_state.list_result=list_result
        print("list result", list_result)
        print(option_model)
        st.session_state.df_result = pd.DataFrame(
            {"model": option_model, "result": list_result})
        y_pred = pipline_cls.predict(test_texts)
        st.session_state.pipsave = pipline_cls

        # st.write("Accuracy on test set: ",accuracy_score(test_labels,y_pred))
        # st.write("√Åp d·ª•ng tr√™n d·ªØ li·ªáu ƒë√£ c√†o ƒë∆∞·ª£c: ",)
        # if option_mission=="setiment analysis":

        #     df=st.session_state.df2
        #     df_title=[]
        #     df_sent=[]
        #     df_result=[]
        #     for idx, row in df.iterrows():
        #         for sentence in sent_tokenize(row["text"]):
        #             df_title.append(row["title"])
        #             df_sent.append(sentence)
        #             df_result.append(predict_sentiment(str(sentence), pipline_cls))
        #     df_sent=pd.DataFrame({"title":df_title,"sentence":df_sent,"result":df_result})

        #     st.dataframe(df_sent)
        #     # st.write(predict_sentiment(example, pipline_cls))
        #     st.session_state.df_step5 = df_sent
        # elif option_mission=="topic classification":
        #     df=st.session_state.df2
        #     df_title=[]
        #     df_sent=[]
        #     df_result=[]
        #     for idx, row in df.iterrows():
        #         for sentence in sent_tokenize(row["text"]):
        #             df_title.append(row["title"])
        #             df_sent.append(sentence)
        #             df_result.append(predict_topic(str(sentence), pipline_cls))
        #     df_sent=pd.DataFrame({"title":df_title,"sentence":df_sent,"result":df_result})
        #     st.session_state.df_step5 = df_sent
        #     st.dataframe(df_sent)
        # elif option_mission=="question classification":
        #     df=st.session_state.df2
        #     df_title=[]
        #     df_sent=[]
        #     df_result=[]
        #     for idx, row in df.iterrows():
        #         for sentence in sent_tokenize(row["text"]):
        #             df_title.append(row["title"])
        #             df_sent.append(sentence)
        #             df_result.append(predict_question(str(sentence), pipline_cls))
        #     df_sent=pd.DataFrame({"title":df_title,"sentence":df_sent,"result":df_result})
        #     st.session_state.df_step5 = df_sent
        #     st.dataframe(df_sent)

    from streamlit_elements import nivo

    if "df_result" in st.session_state:
        fig = px.bar(
            st.session_state.df_result,
            x='model',
            y='result',
            color_discrete_sequence=['skyblue'],
            labels={'model': 'M√¥ h√¨nh', 'result': 'ƒê·ªô ch√≠nh x√°c'},
            title='Bi·ªÉu ƒë·ªì th·ªÉ hi·ªán ƒë·ªô ch√≠nh x√°c c·ªßa c√°c m√¥ h√¨nh khi ki·ªÉm tra tr√™n t·∫≠p ki·ªÉm th·ª≠'
        )

        st.plotly_chart(fig)
        # indexmodel=sac.segmented(
        #     items=option_model,
        #     align="center",return_index=True,
        # )
        boxmodel = st.selectbox("Ch·ªçn m√¥ h√¨nh ƒë·ªÉ th·ª≠ nghi·ªám", option_model)
        print("indexmodel", boxmodel)
        indexmodel = option_model.index(boxmodel)

        example = st.text_area("Nh·∫≠p vƒÉn ph√¢n lo·∫°i")
        btn_button = st.button("Th·ª≠ vƒÉn b·∫£n")

    if 'example' in locals():

        if btn_button:
            pl = st.session_state.list_pipline[indexmodel]
            if option_mission == "setiment analysis":
                st.info("K·∫øt qu·∫£ th·ª≠ "+predict_sentiment(example, pl))
            elif option_mission == "topic classification":
                st.info("K·∫øt qu·∫£ th·ª≠ "+predict_topic(example, pl))
            elif option_mission == "question classification":
                st.info("K·∫øt qu·∫£ th·ª≠ "+predict_question(example, pl))
    btn_thutrendata = st.button("Th·ª≠ tr√™n d·ªØ li·ªáu ƒë√£ c√†o")
    if 'example' in locals():
        if btn_thutrendata:
            # Mapping nhi·ªám v·ª• v·ªõi h√†m x·ª≠ l√Ω
            mission_function_map = {
                "setiment analysis": predict_sentiment,
                "topic classification": predict_topic,
                "question classification": predict_question
            }

            # Ki·ªÉm tra n·∫øu nhi·ªám v·ª• n·∫±m trong map
            if option_mission in mission_function_map:
                pl = st.session_state.list_pipline[indexmodel]
                predict_function = mission_function_map[option_mission]

                df = st.session_state.df2
                df_title = []
                df_sent = []
                df_result = []

                for idx, row in df.iterrows():
                    for sentence in sent_tokenize(row["text"]):
                        df_title.append(row["title"])
                        df_sent.append(sentence)
                        df_result.append(predict_function(str(sentence), pl))

                df_result_df = pd.DataFrame({
                    "title": df_title,
                    "sentence": df_sent,
                    "result": df_result
                })

                def highlight_row(row):
                    if row == "Ti√™u c·ª±c":
                        return ['background-color: red'] * len(row)
                    else:
                        return ['background-color: green'] * len(row)

                def color_result(val):
                    if val == "Ti√™u c·ª±c":
                        return 'background-color: red'
                    else:
                        return 'background-color: green'
                # df_result_df = df_result_df.style.apply(
                #    highlight_row,axis=1
                # )
                # df_result_df["result"] = df_result_df["result"].style.apply(color_result)
                df_result_df = df_result_df.style.applymap(
                    color_result, subset=["result"])
                st.session_state.df_step5 = df_result_df
                st.dataframe(df_result_df, use_container_width=True)


left, middle, right = st.columns(3, vertical_alignment="bottom")
print("step ", st.session_state.step)
# if left.button("Quay l·∫°i",icon="‚¨ÖÔ∏è"):
#     # if st.session_state.step > 1:
#     st.session_state.step -= 1
#     st.rerun()
# if right.button("Ti·∫øp theo" ,icon='‚û°Ô∏è'):
#     # if st.session_state.step <5:
#     u=1+st.session_state.step
#     st.session_state.step = u
#     st.rerun()
# if middle.button("Reset",icon="üîÑ"):
#     st.session_state.step = 1
#     st.rerun()
# bi·ªÉu ƒë·ªì so s√°nh side bar
